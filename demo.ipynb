{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd96faf-b065-4f3a-8858-780d10577830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19dd55c6-0de5-4777-b55e-00656a28258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- io helpers -------------\n",
    "def read_parquet_any(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read parquet using pyarrow if available, else fastparquet.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_parquet(path, engine=\"pyarrow\")\n",
    "    except Exception:\n",
    "        return pd.read_parquet(path, engine=\"fastparquet\")\n",
    "\n",
    "\n",
    "def write_parquet_any(df: pd.DataFrame, path: str) -> None:\n",
    "    \"\"\"\n",
    "    Write parquet using pyarrow if available, else fastparquet.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_parquet(path, engine=\"pyarrow\", index=False)\n",
    "    except Exception:\n",
    "        df.to_parquet(path, engine=\"fastparquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ccb0375-4830-4eab-8f6d-83c61434f4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/xpan02/CASNL/topic_recurrence'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79bf99b-c5f6-4fd2-90f3-3cc7a7f30419",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVO_INPUT_DATA_PATH = '/project/ycleong/datasets/CANDOR'\n",
    "FRIENDS_INPUT_DATA_PATH = '/project/ycleong/datasets/Friends'\n",
    "\n",
    "# input / output files\n",
    "BACKBITER_PARQUET = os.path.join(CONVO_INPUT_DATA_PATH, 'chunk_topic-num.parquet')\n",
    "SURVEY_PARQUET = os.path.join(CONVO_INPUT_DATA_PATH, 'survey.ALL.parquet')\n",
    "\n",
    "FRIENDS_PARQUET = os.path.join(FRIENDS_INPUT_DATA_PATH, 'friends_chunk_topic-num.parquet')\n",
    "\n",
    "backbiter = read_parquet_any(BACKBITER_PARQUET)\n",
    "survey = read_parquet_any(SURVEY_PARQUET)\n",
    "\n",
    "friends = read_parquet_any(FRIENDS_PARQUET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bac761c-0f83-4fc7-8a79-8d7dddf73e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>chunk_text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0020a0c5-1658-4747-99c1-2839e736b481</td>\n",
       "      <td>Mhm. Mhm. Just, mm. And Uh huh, mm. Mhm. Mhm. ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0020a0c5-1658-4747-99c1-2839e736b481</td>\n",
       "      <td>this is actually my first one so uh yeah it's ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0020a0c5-1658-4747-99c1-2839e736b481</td>\n",
       "      <td>thanks, Tiny firm sponge. I swear that's that'...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0020a0c5-1658-4747-99c1-2839e736b481</td>\n",
       "      <td>but I do really like sleep quite a bit and I h...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0020a0c5-1658-4747-99c1-2839e736b481</td>\n",
       "      <td>that sounds really cute. So what are their nam...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_id                       conversation_id  \\\n",
       "0         0  0020a0c5-1658-4747-99c1-2839e736b481   \n",
       "1         1  0020a0c5-1658-4747-99c1-2839e736b481   \n",
       "2         2  0020a0c5-1658-4747-99c1-2839e736b481   \n",
       "3         3  0020a0c5-1658-4747-99c1-2839e736b481   \n",
       "4         4  0020a0c5-1658-4747-99c1-2839e736b481   \n",
       "\n",
       "                                          chunk_text  topic  \n",
       "0  Mhm. Mhm. Just, mm. And Uh huh, mm. Mhm. Mhm. ...     -1  \n",
       "1  this is actually my first one so uh yeah it's ...     -1  \n",
       "2  thanks, Tiny firm sponge. I swear that's that'...     10  \n",
       "3  but I do really like sleep quite a bit and I h...     -1  \n",
       "4  that sounds really cute. So what are their nam...     10  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbiter.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3682eed8-3972-482f-941a-78caaa276c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84358"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(backbiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed0da803-a69c-408a-88a8-4f5c81b2f650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic\n",
       "-1      44809\n",
       " 0       2211\n",
       " 1       1932\n",
       " 2       1830\n",
       " 3       1701\n",
       "        ...  \n",
       " 364       10\n",
       " 365       10\n",
       " 366       10\n",
       " 367       10\n",
       " 368       10\n",
       "Name: count, Length: 370, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbiter[\"topic\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8ce0a03-1f58-4c4f-a595-2170ae41abb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id_orig</th>\n",
       "      <th>n_words</th>\n",
       "      <th>end_turn_id</th>\n",
       "      <th>start_turn_id</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>season_id</th>\n",
       "      <th>scene_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>chunk_text</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>s01_e01</td>\n",
       "      <td>s01</td>\n",
       "      <td>s01_e01_c01</td>\n",
       "      <td>0</td>\n",
       "      <td>There's nothing to tell! He's just some guy I ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>s01_e01</td>\n",
       "      <td>s01</td>\n",
       "      <td>s01_e01_c01</td>\n",
       "      <td>1</td>\n",
       "      <td>Oh, yeah. Had that dream. Then I look down, an...</td>\n",
       "      <td>66</td>\n",
       "      <td>dream, had, dreamt, he, bed, jack, night, saw,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>109</td>\n",
       "      <td>39</td>\n",
       "      <td>26</td>\n",
       "      <td>s01_e01</td>\n",
       "      <td>s01</td>\n",
       "      <td>s01_e01_c01</td>\n",
       "      <td>2</td>\n",
       "      <td>Carol moved her stuff out today. Ohh. Let me g...</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>103</td>\n",
       "      <td>48</td>\n",
       "      <td>40</td>\n",
       "      <td>s01_e01</td>\n",
       "      <td>s01</td>\n",
       "      <td>s01_e01_c01</td>\n",
       "      <td>3</td>\n",
       "      <td>Oh really, so that hysterical phone call I got...</td>\n",
       "      <td>63</td>\n",
       "      <td>married, rachel, annulment, rog, angelica, ros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>112</td>\n",
       "      <td>55</td>\n",
       "      <td>49</td>\n",
       "      <td>s01_e01</td>\n",
       "      <td>s01</td>\n",
       "      <td>s01_e01_c01</td>\n",
       "      <td>4</td>\n",
       "      <td>Oh God Monica hi! Thank God! I just went to yo...</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_id_orig  n_words  end_turn_id  start_turn_id episode_id season_id  \\\n",
       "0              0      117           10              1    s01_e01       s01   \n",
       "1              1      120           25             11    s01_e01       s01   \n",
       "2              2      109           39             26    s01_e01       s01   \n",
       "3              3      103           48             40    s01_e01       s01   \n",
       "4              4      112           55             49    s01_e01       s01   \n",
       "\n",
       "      scene_id  chunk_id                                         chunk_text  \\\n",
       "0  s01_e01_c01         0  There's nothing to tell! He's just some guy I ...   \n",
       "1  s01_e01_c01         1  Oh, yeah. Had that dream. Then I look down, an...   \n",
       "2  s01_e01_c01         2  Carol moved her stuff out today. Ohh. Let me g...   \n",
       "3  s01_e01_c01         3  Oh really, so that hysterical phone call I got...   \n",
       "4  s01_e01_c01         4  Oh God Monica hi! Thank God! I just went to yo...   \n",
       "\n",
       "   topic                                        topic_words  \n",
       "0     -1                                               None  \n",
       "1     66  dream, had, dreamt, he, bed, jack, night, saw,...  \n",
       "2     -1                                               None  \n",
       "3     63  married, rachel, annulment, rog, angelica, ros...  \n",
       "4     -1                                               None  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friends.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0601fb2e-3bfa-471d-b527-083527d20b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7708"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d364ad1e-446b-473c-8323-7e57bbe297de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic\n",
       "-1     4095\n",
       " 0      317\n",
       " 1      279\n",
       " 2      128\n",
       " 3      120\n",
       "       ... \n",
       " 84      11\n",
       " 85      11\n",
       " 86      10\n",
       " 87      10\n",
       " 88      10\n",
       "Name: count, Length: 90, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friends[\"topic\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11305ed3-4c89-46cd-b97a-b924de768778",
   "metadata": {},
   "source": [
    "### Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20191636-44f1-42fc-ace3-a21445191f9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/python-anaconda-2023.09-el8-x86_64/envs/rapids-24.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scripts.parquet_helper import read_parquet_any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbf4ee63-1a04-47c1-bd7b-1c1e78ad1395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: /software/python-anaconda-2023.09-el8-x86_64/envs/rapids-24.12/bin/python\n",
      "transformers: 4.57.1\n",
      "accelerate: 1.12.0\n",
      "huggingface_hub: 0.35.3\n",
      "accelerate spec: ModuleSpec(name='accelerate', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7f88eb307800>, origin='/home/xpan02/.local/lib/python3.12/site-packages/accelerate/__init__.py', submodule_search_locations=['/home/xpan02/.local/lib/python3.12/site-packages/accelerate'])\n"
     ]
    }
   ],
   "source": [
    "import sys, importlib.util\n",
    "import transformers, accelerate, huggingface_hub\n",
    "\n",
    "print(\"python:\", sys.executable)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"accelerate:\", accelerate.__version__)\n",
    "print(\"huggingface_hub:\", huggingface_hub.__version__)\n",
    "print(\"accelerate spec:\", importlib.util.find_spec(\"accelerate\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d2f640a-0ee5-4ec3-88b6-b47e9afb2c94",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m model_id = \u001b[33m\"\u001b[39m\u001b[33mmeta-llama/Llama-3.3-70B-Instruct\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m bnb = BitsAndBytesConfig(load_in_8bit=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m tok = AutoTokenizer.from_pretrained(model_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:4881\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4872\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m transformers_explicit_filename.endswith(\n\u001b[32m   4873\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m.safetensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4874\u001b[39m     ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m transformers_explicit_filename.endswith(\u001b[33m\"\u001b[39m\u001b[33m.safetensors.index.json\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   4875\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4876\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe transformers file in the config seems to be incorrect: it is neither a safetensors file \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4877\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m(*.safetensors) nor a safetensors index file (*.safetensors.index.json): \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4878\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_explicit_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   4879\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m4881\u001b[39m hf_quantizer, config, dtype, device_map = \u001b[43mget_hf_quantizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\n\u001b[32m   4883\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4885\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4886\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4887\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4888\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/quantizers/auto.py:319\u001b[39m, in \u001b[36mget_hf_quantizer\u001b[39m\u001b[34m(config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent)\u001b[39m\n\u001b[32m    316\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     dtype = hf_quantizer.update_dtype(dtype)\n\u001b[32m    327\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_8bit.py:73\u001b[39m, in \u001b[36mBnb8BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     70\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m     )\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available(check_library_only=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     74\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m     )\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     78\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe bitsandbytes library requires PyTorch but it was not found in your environment. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     79\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can install it with `pip install torch`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     80\u001b[39m     )\n",
      "\u001b[31mImportError\u001b[39m: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "bnb = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    "    quantization_config=bnb,\n",
    ")\n",
    "tok = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddcc3c3-f4cc-4211-adb7-44d8af5532d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_CSV = \"topic-label_all.csv\"\n",
    "\n",
    "N_PER_TOPIC = 10\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6bd89b3c-8c56-4cc0-bd64-3f8da6eacd63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>chunk_text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0020a0c5-1658-4747-99c1-2839e736b481</td>\n",
       "      <td>Mhm. Mhm. Just, mm. And Uh huh, mm. Mhm. Mhm. ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0020a0c5-1658-4747-99c1-2839e736b481</td>\n",
       "      <td>this is actually my first one so uh yeah it's ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0020a0c5-1658-4747-99c1-2839e736b481</td>\n",
       "      <td>thanks, Tiny firm sponge. I swear that's that'...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0020a0c5-1658-4747-99c1-2839e736b481</td>\n",
       "      <td>but I do really like sleep quite a bit and I h...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0020a0c5-1658-4747-99c1-2839e736b481</td>\n",
       "      <td>that sounds really cute. So what are their nam...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_id                       conversation_id  \\\n",
       "0         0  0020a0c5-1658-4747-99c1-2839e736b481   \n",
       "1         1  0020a0c5-1658-4747-99c1-2839e736b481   \n",
       "2         2  0020a0c5-1658-4747-99c1-2839e736b481   \n",
       "3         3  0020a0c5-1658-4747-99c1-2839e736b481   \n",
       "4         4  0020a0c5-1658-4747-99c1-2839e736b481   \n",
       "\n",
       "                                          chunk_text  topic  \n",
       "0  Mhm. Mhm. Just, mm. And Uh huh, mm. Mhm. Mhm. ...     -1  \n",
       "1  this is actually my first one so uh yeah it's ...     -1  \n",
       "2  thanks, Tiny firm sponge. I swear that's that'...     10  \n",
       "3  but I do really like sleep quite a bit and I h...     -1  \n",
       "4  that sounds really cute. So what are their nam...     10  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbiter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4cb757a0-9e61-4a00-9c24-dd835e8d1af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-1, 0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
       " [359, 360, 361, 362, 363, 364, 365, 366, 367, 368])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = sorted(backbiter[\"topic\"].dropna().unique().tolist())\n",
    "topics[:10], topics[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "46795a03-af66-473b-9be0-39298cac2e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(370, 369)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_no_noise = [t for t in topics if t != -1]\n",
    "len(topics), len(topics_no_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bef15508-676a-449c-9ff2-c38be55dc36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_topic_texts(group: pd.DataFrame, n: int = 20, seed: int = 42) -> list[str]:\n",
    "    return (\n",
    "        group[\"chunk_text\"]\n",
    "        .dropna()\n",
    "        .astype(str)\n",
    "        .sample(n=min(n, len(group)), random_state=seed)\n",
    "        .tolist()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d011dcd8-14da-4c41-bf15-e5e90e936083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,\n",
       " \"people just don't know as much, so it's better for them to take their time and figure it out rather than just like going, spending too much money on school and having to be Square Hey stomach? No. Yea\")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = topics_no_noise[0]\n",
    "group = backbiter[backbiter[\"topic\"] == t]\n",
    "examples = sample_topic_texts(group, n=N_PER_TOPIC, seed=SEED)\n",
    "\n",
    "len(examples), examples[0][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "85918c38-0461-495e-9ed2-f0558f9f5e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "You are an expert annotator analyzing a latent conversation topic.\n",
    "All the text chunks below come from the same topic.\n",
    "\n",
    "### Topic ID: {topic_id}\n",
    "\n",
    "### Example text chunks\n",
    "{chunk_examples}\n",
    "\n",
    "### Task\n",
    "Based on these examples, infer the underlying topic.\n",
    "Produce only a one-row Markdown table with:\n",
    "\n",
    "- topic_id: {topic_id}\n",
    "- short_label: a concise 2–5 word name\n",
    "- summary: one sentence describing what people are doing or discussing in this topic\n",
    "- keywords: 3–8 key words or phrases (comma separated)\n",
    "\n",
    "### Output format (very important)\n",
    "| topic_id | short_label | summary | keywords |\n",
    "|----------|-------------|---------|----------|\n",
    "| {topic_id} | ... | ... | ... |\n",
    "\n",
    "Do not add extra commentary.\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "196ce947-a95d-4df8-9c9c-f726852319df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert annotator analyzing a latent conversation topic.\n",
      "All the text chunks below come from the same topic.\n",
      "\n",
      "### Topic ID: 0\n",
      "\n",
      "### Example text chunks\n",
      "- 1. people just don't know as much, so it's better for them to take their time and figure it out rather than just like going, spending too much money on school and having to be Square Hey stomach? No. Yeah. Yeah. The bad thing about having cats in the apartment is they like to destroy everything? What? Yeah. crowd Oh my God, she's so bad. Hi. She's already destroyed All all the blinds in my apartment. Will not off. There's still one that's All going strong but I know for sure I have to replace the risk when I'm going away. Okay. apartment blinds don't really hold up very well.\n",
      "- 2. I don't know, I'm scared for him because only because like when he sleeps, he snores like a person, I'm like, oh God, I'm like, it's getting worse. I'm like, it's because we also spayed or neutered, I'm not sure what the term is, but we did that to him and ever since then he got really big and I didn't know it does that to them. I was like wow two.\n",
      "- 3. I'm very sorry to them. They didn't do anything wrong, but you're nuts. Yeah. You know the dogs are like mhm. They were like, please take out one of you. They're really crazy names to. They were like, really long. She was from brazil while we're there. One of the names. I always remember something so long. It was like, remember keto genia Yeah, I will be very junior. that's your new name. Y\n"
     ]
    }
   ],
   "source": [
    "def build_prompt(topic_id: int, chunk_examples: list[str]) -> str:\n",
    "    chunk_examples_block = \"\\n\".join([f\"- {i+1}. {text}\" for i, text in enumerate(chunk_examples)])\n",
    "    return PROMPT.format(topic_id=topic_id, chunk_examples=chunk_examples_block)\n",
    "\n",
    "prompt = build_prompt(t, examples)\n",
    "print(prompt[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301ab1e-6285-40f2-939d-0e598c72ff20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33983a73-a3d4-45d5-b1da-9ab073942771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c9184b-8893-4e6b-98bb-4c009ee98aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rapids-24.12)",
   "language": "python",
   "name": "rapids-24.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
