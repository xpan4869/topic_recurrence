import os
import duckdb
import pandas as pd
import numpy as np
from pathlib import Path


# ------------- io helpers -------------
def read_parquet_any(path: str) -> pd.DataFrame:
    """
    Read parquet using pyarrow if available, else fastparquet.
    """
    try:
        return pd.read_parquet(path, engine="pyarrow")
    except Exception:
        return pd.read_parquet(path, engine="fastparquet")


def write_parquet_any(df: pd.DataFrame, path: str) -> None:
    """
    Write parquet using pyarrow if available, else fastparquet.
    """
    try:
        df.to_parquet(path, engine="pyarrow", index=False)
    except Exception:
        df.to_parquet(path, engine="fastparquet", index=False)



os.getcwd()


CONVO_INPUT_DATA_PATH = '/project/ycleong/datasets/CANDOR'
FRIENDS_INPUT_DATA_PATH = '/project/ycleong/datasets/Friends'

# input / output files
BACKBITER_PARQUET = os.path.join(CONVO_INPUT_DATA_PATH, 'chunk_topic.parquet')
SURVEY_PARQUET = os.path.join(CONVO_INPUT_DATA_PATH, 'survey.ALL.parquet')

FRIENDS_PARQUET = os.path.join(FRIENDS_INPUT_DATA_PATH, 'friends_chunk_topic.parquet')

backbiter = read_parquet_any(BACKBITER_PARQUET)
survey = read_parquet_any(SURVEY_PARQUET)

friends = read_parquet_any(FRIENDS_PARQUET)


backbiter.head()



len(backbiter)


backbiter["topic"].value_counts().sort_index()


friends.head()


len(friends)


friends["topic"].value_counts().sort_index()





import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import json
import numpy as np
import os
from pathlib import Path
import pandas as pd
from tqdm import tqdm

from scripts.parquet_helper import read_parquet_any


import sys, importlib.util
import transformers, accelerate, huggingface_hub

print("python:", sys.executable)
print("transformers:", transformers.__version__)
print("accelerate:", accelerate.__version__)
print("huggingface_hub:", huggingface_hub.__version__)
print("accelerate spec:", importlib.util.find_spec("accelerate"))



model_id = "meta-llama/Llama-3.3-70B-Instruct"
bnb = BitsAndBytesConfig(load_in_8bit=True)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    dtype=torch.bfloat16,
    quantization_config=bnb,
)
tok = AutoTokenizer.from_pretrained(model_id)


OUT_CSV = "topic-label_all.csv"

N_PER_TOPIC = 10
SEED = 42


backbiter.head()


topics = sorted(backbiter["topic"].dropna().unique().tolist())
topics[:10], topics[-10:]


topics_no_noise = [t for t in topics if t != -1]
len(topics), len(topics_no_noise)


def sample_topic_texts(group: pd.DataFrame, n: int = 20, seed: int = 42) -> list[str]:
    return (
        group["chunk_text"]
        .dropna()
        .astype(str)
        .sample(n=min(n, len(group)), random_state=seed)
        .tolist()
    )


t = topics_no_noise[0]
group = backbiter[backbiter["topic"] == t]
examples = sample_topic_texts(group, n=N_PER_TOPIC, seed=SEED)

len(examples), examples[0][:200]


PROMPT = """
You are an expert annotator analyzing a latent conversation topic.
All the text chunks below come from the same topic.

### Topic ID: {topic_id}

### Example text chunks
{chunk_examples}

### Task
Based on these examples, infer the underlying topic.
Produce only a one-row Markdown table with:

- topic_id: {topic_id}
- short_label: a concise 2–5 word name
- summary: one sentence describing what people are doing or discussing in this topic
- keywords: 3–8 key words or phrases (comma separated)

### Output format (very important)
| topic_id | short_label | summary | keywords |
|----------|-------------|---------|----------|
| {topic_id} | ... | ... | ... |

Do not add extra commentary.
""".strip()



def build_prompt(topic_id: int, chunk_examples: list[str]) -> str:
    chunk_examples_block = "\n".join([f"- {i+1}. {text}" for i, text in enumerate(chunk_examples)])
    return PROMPT.format(topic_id=topic_id, chunk_examples=chunk_examples_block)

prompt = build_prompt(t, examples)
print(prompt[:1500])









